[["Map",1,2,9,10],"meta::meta",["Map",3,4,5,6,7,8],"astro-version","5.1.1","content-config-digest","52892818892996ca","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"site\":\"https://meej.ca\",\"compressHTML\":true,\"base\":\"/\",\"trailingSlash\":\"ignore\",\"output\":\"static\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":false,\"port\":4321,\"streaming\":true},\"redirects\":{},\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[],\"remotePatterns\":[]},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":\"shiki\",\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"github-dark\",\"themes\":{},\"wrap\":false,\"transformers\":[]},\"remarkPlugins\":[],\"rehypePlugins\":[],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"responsiveImages\":false},\"legacy\":{\"collections\":false}}","project",["Map",11,12,39,40,69,70,90,91],"autonomous-mario-kart-robot",{"id":11,"data":13,"body":18,"filePath":19,"digest":20,"rendered":21},{"title":14,"description":15,"slug":11,"order":16,"link":17},"Autonomous Mario Kart Robot","Driving around a perilous terrain, no one to help...",3,"https://github.com/AmjadYa/Autonomous-Robot","\u003Cdiv class=\"flex gap-2 flex-wrap sm:flex-nowrap\">\r\n    \u003Cvideo src=\"/videos/robot1.mp4\" muted style=\"max-height:400px ; aspect-ratio:1; object-fit:cover\" controls>\u003C/video>\r\n    \u003Cimg src=\"/images/robot_on_zipline.jpg\" style=\"max-height:400px ; aspect-ratio:1 ; object-fit:cover\">\r\n\u003C/div>\r\n\r\n## Summary\r\n\r\nIn the summer of 2023, I had to build a robot that could navigate an obstacle course, collect coins off the ground and complete laps for points - with no human interference. Since we thought things weren't hard enough, our team was **one of two** to implement a zipline mechanism to short-cut a part of the course.\r\n\r\nWe split the work up into three disciplines: Electrical, Hardware and Software and progressively integrated components together.\r\n\r\n## Electrical\r\n\r\nThe biggest electrical challenge was getting the right power to all our motors. We had 5V motors, 3V pins, beefy 15V DC motors and an STM-32 Blue Pill (our robot's brain) all supplied from one 15V and one 9V lithium ion battery. We soldered a robust power distribution board with a combination of buck converters and voltage dividers. Due the high currents and magnetic fields caused by the DC motors we had to curl our external wires and take care to avoid noise.\r\n\r\nAnother note is that the Blue Pill has incredibly sensitive pins (!!!). We used our oscilloscope countless times for troubleshooting pin-issues and power failures. I learned how delicate the electronics on low-power systems are.\r\n\r\n\u003Cdiv class=\"flex flex-wrap sm:flex-nowrap gap-2\">\r\n    \u003Cimg src=\"/images/h bridge.jpg\" style=\"max-height:400px ; aspect-ratio:1 ; object-fit:cover\">\r\n    \u003Cimg src=\"/images/wired up.jpg\" style=\"max-height:400px ; aspect-ratio:1 ; object-fit:cover\">\r\n\u003C/div>\r\n\r\n## Hardware\r\n\r\nOur drive base utilized Ackerman steering, with a servo directing the front wheels and DC motors powering the rear wheels. The chassis was made out of lasercut plywood, acrylic and 3d printed parts. The zipline mechanism was designed so that the roller wheels interlock into each other like a zipper. This meant the reaction force from contact with the beam would help the claw stay shut. Once the robot touched the ground, that reaction component would disappear and we could safely open the claw again.\r\n\r\n\u003Cdiv class=\"flex flex-wrap sm:flex-nowrap gap-2\">\r\n    \u003Cimg src=\"/images/robotcad1.jpg\" style=\"max-height:400px ; aspect-ratio:1 ; object-fit:cover\">\r\n    \u003Cimg src=\"/images/robotcad2.jpg\" style=\"max-height:400px ; aspect-ratio:1 ; object-fit:cover\">\r\n\u003C/div>\r\n\r\n## Software / Firmware\r\n\r\nThis project was where I fell in love with firmware; it's fun turning abstract instructions into actions.\r\n\r\nWe implemented everything using Arduino. My favorite feature was a convolution algorithm that processed input from a 1kHz infrared beacon located at the end of the track. This enabled us to detect the desired light signal amidst potential noise and other IR sources. We sampled and normalized the IR data from front-mounted sensors, convolved it with a predefined 1kHz wave and applied a threshold to the resulting sum to determine whether the beacon was detected.\r\n\r\nThe whole robot operated through a multi-stage loop: initially following IR signals then 90 degree turns, PID steering up a ramp, ziplining down and restarting. Additionally, we used hardware interrupts to detect edges and execute maneuvers like backing up or making sharp turns before falling off the edge.\r\n\r\nPutting the code together and getting to see the fruits of all the hardware-labour was satisfying.","src/data/project/autonomous-mario-kart-robot.md","d1df3fb74495d8b5",{"html":22,"metadata":23},"\u003Cdiv class=\"flex gap-2 flex-wrap sm:flex-nowrap\">\n    \u003Cvideo src=\"/videos/robot1.mp4\" muted style=\"max-height:400px ; aspect-ratio:1; object-fit:cover\" controls>\u003C/video>\n    \u003Cimg src=\"/images/robot_on_zipline.jpg\" style=\"max-height:400px ; aspect-ratio:1 ; object-fit:cover\">\n\u003C/div>\n\u003Ch2 id=\"summary\">Summary\u003C/h2>\n\u003Cp>In the summer of 2023, I had to build a robot that could navigate an obstacle course, collect coins off the ground and complete laps for points - with no human interference. Since we thought things weren’t hard enough, our team was \u003Cstrong>one of two\u003C/strong> to implement a zipline mechanism to short-cut a part of the course.\u003C/p>\n\u003Cp>We split the work up into three disciplines: Electrical, Hardware and Software and progressively integrated components together.\u003C/p>\n\u003Ch2 id=\"electrical\">Electrical\u003C/h2>\n\u003Cp>The biggest electrical challenge was getting the right power to all our motors. We had 5V motors, 3V pins, beefy 15V DC motors and an STM-32 Blue Pill (our robot’s brain) all supplied from one 15V and one 9V lithium ion battery. We soldered a robust power distribution board with a combination of buck converters and voltage dividers. Due the high currents and magnetic fields caused by the DC motors we had to curl our external wires and take care to avoid noise.\u003C/p>\n\u003Cp>Another note is that the Blue Pill has incredibly sensitive pins (!!!). We used our oscilloscope countless times for troubleshooting pin-issues and power failures. I learned how delicate the electronics on low-power systems are.\u003C/p>\n\u003Cdiv class=\"flex flex-wrap sm:flex-nowrap gap-2\">\n    \u003Cimg src=\"/images/h bridge.jpg\" style=\"max-height:400px ; aspect-ratio:1 ; object-fit:cover\">\n    \u003Cimg src=\"/images/wired up.jpg\" style=\"max-height:400px ; aspect-ratio:1 ; object-fit:cover\">\n\u003C/div>\n\u003Ch2 id=\"hardware\">Hardware\u003C/h2>\n\u003Cp>Our drive base utilized Ackerman steering, with a servo directing the front wheels and DC motors powering the rear wheels. The chassis was made out of lasercut plywood, acrylic and 3d printed parts. The zipline mechanism was designed so that the roller wheels interlock into each other like a zipper. This meant the reaction force from contact with the beam would help the claw stay shut. Once the robot touched the ground, that reaction component would disappear and we could safely open the claw again.\u003C/p>\n\u003Cdiv class=\"flex flex-wrap sm:flex-nowrap gap-2\">\n    \u003Cimg src=\"/images/robotcad1.jpg\" style=\"max-height:400px ; aspect-ratio:1 ; object-fit:cover\">\n    \u003Cimg src=\"/images/robotcad2.jpg\" style=\"max-height:400px ; aspect-ratio:1 ; object-fit:cover\">\n\u003C/div>\n\u003Ch2 id=\"software--firmware\">Software / Firmware\u003C/h2>\n\u003Cp>This project was where I fell in love with firmware; it’s fun turning abstract instructions into actions.\u003C/p>\n\u003Cp>We implemented everything using Arduino. My favorite feature was a convolution algorithm that processed input from a 1kHz infrared beacon located at the end of the track. This enabled us to detect the desired light signal amidst potential noise and other IR sources. We sampled and normalized the IR data from front-mounted sensors, convolved it with a predefined 1kHz wave and applied a threshold to the resulting sum to determine whether the beacon was detected.\u003C/p>\n\u003Cp>The whole robot operated through a multi-stage loop: initially following IR signals then 90 degree turns, PID steering up a ramp, ziplining down and restarting. Additionally, we used hardware interrupts to detect edges and execute maneuvers like backing up or making sharp turns before falling off the edge.\u003C/p>\n\u003Cp>Putting the code together and getting to see the fruits of all the hardware-labour was satisfying.\u003C/p>",{"headings":24,"imagePaths":38,"frontmatter":13},[25,29,32,35],{"depth":26,"slug":27,"text":28},2,"summary","Summary",{"depth":26,"slug":30,"text":31},"electrical","Electrical",{"depth":26,"slug":33,"text":34},"hardware","Hardware",{"depth":26,"slug":36,"text":37},"software--firmware","Software / Firmware",[],"projection-mapped-interactive-string",{"id":39,"data":41,"body":46,"filePath":47,"digest":48,"rendered":49},{"title":42,"description":43,"slug":39,"order":44,"link":45},"Projection-Mapped Interactive String","Using OpenCV and a projector to play nice chords.",1,"https://github.com/AmjadYa/interactive-string","## Demo Video\r\n\r\n\u003Cdiv class=\"flex gap-2\" style=\"justify-content: center ; align-items: center\">\r\n    \u003Cvideo src=\"/videos/pluck with glove.mp4\" place-content-center muted style=\"max-height:400px ; object-fit:cover\" controls>\u003C/video>\r\n\u003C/div>\r\n\r\n_Make sure you unmute the video if you want to hear what it sounds like!_\r\n\r\n## The Vision\r\n\r\nEver since I learned about projection mapping, I have been wanting to try it for myself. I'm particularly inspired by this creator named \u003Ca href=\"https://www.instagram.com/roelofknol/?hl=en\" target=\"_blank\">Roelof Knol\u003C/a>, who's projections are always interactive. In this project, I wanted to project a guitar string on my wall and touch its image to play nice chords.\r\n\r\n\u003Cdiv class=\"flex gap-2\" style=\"justify-content: center ; align-items: center\">\r\n    \u003Cimg src=\"/images/the string.png\" style=\"max-height:400px ; object-fit:cover\">\r\n\u003C/div>\r\n\r\nI chose muted off-white colours to make it look mysterious...\r\n\r\n## Current Progress\r\n\r\nIt works nicely with a glove! I have to play around with thresholding and lighting a little more to get it to work with just my finger. Check out the video at the top of this document to see what it looks like currently.\r\n\r\n### The String\r\n\r\nI used Processing to create a virtual string. If a contour gets close to the string it plays a chord and vibrates. Cute little particles emenate from the string each time it's plucked too.\r\n\r\nChords are randomly picked from a bank I created, however, susequent chords are picked using strategies inspired by common practices in music theory. After the string is plucked, one of seven strategies is picked. The chord pool is then re-analyzed for chords that fit the criteria defined by the selected strategy. Occasionally when plucked, the string ignores the strategy and picks a random chord (so we don't get stuck in loops).\r\n\r\nCheck out \"pluck5.pde\" from the \u003Ca href=\"https://github.com/AmjadYa/interactive-string\" target=\"_blank\">GitHub repository\u003C/a>.\r\n\r\n### Detecting When It's Plucked\r\n\r\nI convert my camera feed into binary (black and white) and detect the largest black spaces. (In reality I invert the image and detect the largest white contours.) When a thin shadow (your finger) passes over the string it _plucks_ the string starting at the x-value of the contour.\r\n\r\nCheck out \"center_contours.pde\" from the \u003Ca href=\"https://github.com/AmjadYa/interactive-string\" target=\"_blank\">GitHub repository\u003C/a>.","src/data/project/projection-mapped-interactive-string.md","42c9c549d2b54361",{"html":50,"metadata":51},"\u003Ch2 id=\"demo-video\">Demo Video\u003C/h2>\n\u003Cdiv class=\"flex gap-2\" style=\"justify-content: center ; align-items: center\">\n    \u003Cvideo src=\"/videos/pluck with glove.mp4\" place-content-center=\"\" muted style=\"max-height:400px ; object-fit:cover\" controls>\u003C/video>\n\u003C/div>\n\u003Cp>\u003Cem>Make sure you unmute the video if you want to hear what it sounds like!\u003C/em>\u003C/p>\n\u003Ch2 id=\"the-vision\">The Vision\u003C/h2>\n\u003Cp>Ever since I learned about projection mapping, I have been wanting to try it for myself. I’m particularly inspired by this creator named \u003Ca href=\"https://www.instagram.com/roelofknol/?hl=en\" target=\"_blank\">Roelof Knol\u003C/a>, who’s projections are always interactive. In this project, I wanted to project a guitar string on my wall and touch its image to play nice chords.\u003C/p>\n\u003Cdiv class=\"flex gap-2\" style=\"justify-content: center ; align-items: center\">\n    \u003Cimg src=\"/images/the string.png\" style=\"max-height:400px ; object-fit:cover\">\n\u003C/div>\n\u003Cp>I chose muted off-white colours to make it look mysterious…\u003C/p>\n\u003Ch2 id=\"current-progress\">Current Progress\u003C/h2>\n\u003Cp>It works nicely with a glove! I have to play around with thresholding and lighting a little more to get it to work with just my finger. Check out the video at the top of this document to see what it looks like currently.\u003C/p>\n\u003Ch3 id=\"the-string\">The String\u003C/h3>\n\u003Cp>I used Processing to create a virtual string. If a contour gets close to the string it plays a chord and vibrates. Cute little particles emenate from the string each time it’s plucked too.\u003C/p>\n\u003Cp>Chords are randomly picked from a bank I created, however, susequent chords are picked using strategies inspired by common practices in music theory. After the string is plucked, one of seven strategies is picked. The chord pool is then re-analyzed for chords that fit the criteria defined by the selected strategy. Occasionally when plucked, the string ignores the strategy and picks a random chord (so we don’t get stuck in loops).\u003C/p>\n\u003Cp>Check out “pluck5.pde” from the \u003Ca href=\"https://github.com/AmjadYa/interactive-string\" target=\"_blank\">GitHub repository\u003C/a>.\u003C/p>\n\u003Ch3 id=\"detecting-when-its-plucked\">Detecting When It’s Plucked\u003C/h3>\n\u003Cp>I convert my camera feed into binary (black and white) and detect the largest black spaces. (In reality I invert the image and detect the largest white contours.) When a thin shadow (your finger) passes over the string it \u003Cem>plucks\u003C/em> the string starting at the x-value of the contour.\u003C/p>\n\u003Cp>Check out “center_contours.pde” from the \u003Ca href=\"https://github.com/AmjadYa/interactive-string\" target=\"_blank\">GitHub repository\u003C/a>.\u003C/p>",{"headings":52,"imagePaths":68,"frontmatter":41},[53,56,59,62,65],{"depth":26,"slug":54,"text":55},"demo-video","Demo Video",{"depth":26,"slug":57,"text":58},"the-vision","The Vision",{"depth":26,"slug":60,"text":61},"current-progress","Current Progress",{"depth":16,"slug":63,"text":64},"the-string","The String",{"depth":16,"slug":66,"text":67},"detecting-when-its-plucked","Detecting When It’s Plucked",[],"breaking-blood-clots-in-zero-g",{"id":69,"data":71,"body":76,"filePath":77,"digest":78,"rendered":79},{"title":72,"description":73,"slug":69,"order":74,"link":75},"Breaking Blood Clots in Zero G","Studying clot behaviour for future space mission safety.",4,"https://github.com/AmjadYa/UBC-Rocket-GUI","\u003Cimg class=\"mx-auto\" src=\"/images/UBCRocket_TeamPhoto.JPG\" style=\"max-height:400px ; object-fit:cover\">\r\n\r\n## You Don't Mean Zero Gravity... Right?\r\n\r\nI do! This project was made possible through a partnership with \u003Ca href=\"https://www.seds.ca/can-rgx/\" target=\"_blank\">CAN-RGX\u003C/a> and the UBC Rocket Payloads team (we look so good in that picture). Our motivation was to determine whether thrombolytic drugs (clot-breaking chemicals) function effectively in outer space. We were doing our part in the ongoing efforts to protect astronauts during deep space missions.\r\n\r\nYou can view a summary of the year-long project here:\r\n\r\n\u003Cdiv style=\"text-align: center; margin-top: 20px;\">\r\n    \u003Ch3>Critical Design Review Slides\u003C/h3>\r\n    \u003Cp>\u003Ca href=\"/pdfs/CDR Slides.pdf\" target=\"_blank\">View Fullscreen\u003C/a>\u003C/p>\r\n    \u003Ciframe src=\"/pdfs/CDR Slides.pdf\" style=\"width: 100%; height: 80vh; mx-auto\">\u003C/iframe>\r\n\u003C/div>\r\n\r\n## The Test\r\n\r\nTwo lucky members from our team got on a parabolic flight and ran our test! Due to timing conflicts with my previous project, I wasn't one of the two - but check out this really cool \u003Ca href=\"https://www.linkedin.com/posts/ubc-rocket_ubc-canrgx-activity-7109263823215218688-7RUi/?utm_source=share&utm_medium=member_desktop \" target=\"_blank\">post about the experience\u003C/a>.","src/data/project/breaking-blood-clots-in-zero-g.md","bb6dd46940380523",{"html":80,"metadata":81},"\u003Cimg class=\"mx-auto\" src=\"/images/UBCRocket_TeamPhoto.JPG\" style=\"max-height:400px ; object-fit:cover\">\n\u003Ch2 id=\"you-dont-mean-zero-gravity-right\">You Don’t Mean Zero Gravity… Right?\u003C/h2>\n\u003Cp>I do! This project was made possible through a partnership with \u003Ca href=\"https://www.seds.ca/can-rgx/\" target=\"_blank\">CAN-RGX\u003C/a> and the UBC Rocket Payloads team (we look so good in that picture). Our motivation was to determine whether thrombolytic drugs (clot-breaking chemicals) function effectively in outer space. We were doing our part in the ongoing efforts to protect astronauts during deep space missions.\u003C/p>\n\u003Cp>You can view a summary of the year-long project here:\u003C/p>\n\u003Cdiv style=\"text-align: center; margin-top: 20px;\">\n    \u003Ch3>Critical Design Review Slides\u003C/h3>\n    \u003Cp>\u003Ca href=\"/pdfs/CDR Slides.pdf\" target=\"_blank\">View Fullscreen\u003C/a>\u003C/p>\n    \u003Ciframe src=\"/pdfs/CDR Slides.pdf\" style=\"width: 100%; height: 80vh; mx-auto\">\u003C/iframe>\n\u003C/div>\n\u003Ch2 id=\"the-test\">The Test\u003C/h2>\n\u003Cp>Two lucky members from our team got on a parabolic flight and ran our test! Due to timing conflicts with my previous project, I wasn’t one of the two - but check out this really cool \u003Ca href=\"https://www.linkedin.com/posts/ubc-rocket_ubc-canrgx-activity-7109263823215218688-7RUi/?utm_source=share&#x26;utm_medium=member_desktop \" target=\"_blank\">post about the experience\u003C/a>.\u003C/p>",{"headings":82,"imagePaths":89,"frontmatter":71},[83,86],{"depth":26,"slug":84,"text":85},"you-dont-mean-zero-gravity-right","You Don’t Mean Zero Gravity… Right?",{"depth":26,"slug":87,"text":88},"the-test","The Test",[],"cnn-letter-detecting-ros-sim",{"id":90,"data":92,"body":96,"filePath":97,"digest":98,"rendered":99},{"title":93,"description":94,"slug":90,"order":26,"link":95},"CNN Letter Detecting ROS Sim","Teacher kept telling me that I'm just a neural network.","https://github.com/AmjadYa/fizzcomp","\u003Cdiv class=\"flex gap-2\" style=\"justify-content: center ; align-items: center\">\r\n    \u003Cvideo src=\"/videos/CNN clue detecting robot.mp4\" place-content-center muted style=\"max-height:400px ; object-fit:cover\" controls>\u003C/video>\r\n\u003C/div>\r\n\r\n\u003Ch2>Introduction\u003C/h2>\r\n\r\n\u003Ch3>Background of the Report\u003C/h3>\r\n\u003Cp>\r\nThis project was the culmination of \u003Cstrong>eight\u003C/strong> labs that provided the essential background to develop a fully autonomous, line-following robot for the semester's end competition. This report details the development and implementation of a control system and Convolutional Neural Network (CNN) enabling the robot to detect and read clue plates, navigate a predefined track and avoid obstacles. The labs were very useful! For a personal logbook of all labs and related thoughts, you can click \u003Ca href=\"/pdfs/Amjad%20Yaghi%20individual%20logbook.pdf\" target=\"_blank\">here\u003C/a>.\r\n\u003C/p>\r\n\r\n\u003Ch3>Contribution Split\u003C/h3>\r\n\u003Cp>\r\nRichard Helper: PID-based line following and obstacle avoidance.\u003Cbr>\r\nAmjad Yaghi: CNN-based clue detection, visual processing of clue boards, UI development, emergency teleportation strategy for competition.\r\n\u003C/p>\r\n\r\n\u003Ch3>Software Architecture\u003C/h3>\r\n\u003Cp>\r\n\u003Cimg src=\"/images/controller.drawio.png\" alt=\"Driving software architecture\" style=\"max-width:80%; display:block; margin:auto;\" />\r\n\u003C/p>\r\n\u003Cp style=\"text-align:center;\">\r\n\u003Ca href=\"https://github.com/OdysseusInSpace/ENPH353_LineFollow\" target=\"_blank\">Driving software architecture\u003C/a>\r\n\u003C/p>\r\n\r\n\u003Cp>\r\n\u003Cimg src=\"/images/gui3.drawio.png\" alt=\"Software architecture used on competition day\" style=\"max-width:80%; display:block; margin:auto;\" />\r\n\u003C/p>\r\n\u003Cp style=\"text-align:center;\">\r\n\u003Ca href=\"https://github.com/AmjadYa/fizzcomp/tree/main\" target=\"_blank\">Software architecture used on competition day\u003C/a>\r\n\u003C/p>\r\n\r\n\u003Chr />\r\n\r\n\u003Ch2>Discussion\u003C/h2>\r\n\r\n\u003Ch3>Robot Driving Method\u003C/h3>\r\n\u003Cp>\r\nThe planned driving module used PID, a state machine, and several specific behaviours to make its way through the obstacle course. Unfortunately, we were not able to integrate it with the clue detection in time, and so were forced to go through with our emergency controller detailed at the end of the section.\r\n\u003C/p>\r\n\r\n\u003Ch4>Contour Detection and Basic PID\u003C/h4>\r\n\u003Cp>\r\nDue to the roundabout in the middle of the on-road section, the robot needed to be biased toward turning left so as not to collide head on with the truck. There was also the issue of noise in the middle of the road during the dirt phase. To counteract these issues, the PID was based off the centre of the leftmost contour above a certain perimeter threshold, using an adaptive offset towards the opposite end of the screen that grew smaller as the marker was placed closer to the horizon. See the figure below for an example.\r\n\u003C/p>\r\n\u003Cp>\r\n\u003Cimg src=\"/images/Dirt_Contour.png\" alt=\"Contour detection and PID marker\" style=\"max-width:50%; display:block; margin:auto;\" />\r\n\u003C/p>\r\n\r\n\u003Ch4>Staging\u003C/h4>\r\n\u003Cp>\r\nAs the robot drives, it encounters different scenarios marked by lines across the road. There is a red bar that marks the beginning of the pedestrian section and pink ones that mark the transition between the paved road, dirt road, offroad, and hill sections. The drive module uses a state machine to account for this. Whenever one of these lines is detected – that is, the bottom 10% of the screen has a certain number of pixels that satisfy the red-pink masking – and then disappears, the next state is initialized. This usually means swapping image processing and control algorithms, as is detailed later in this section.\r\n\u003C/p>\r\n\r\n\u003Ch4>NPC Avoidance\u003C/h4>\r\n\u003Cp>\r\nOf the three NPCs, the pedestrian was the only one that the robot would ever hit. The truck acted as a contour in such a way that the PID automatically avoided it, and Baby Yoda was navigated around entirely.\u003Cbr/>\r\nTo dodge the pedestrian, the staging system was put to simple use. Upon crossing the red line, the robot swaps to phase 2, and waits for the marker to shift dramatically. As the pedestrian crosses the left side of the road, it would disrupt the contour and cause this shift. The robot would then resume its normal behaviour.\r\n\u003C/p>\r\n\u003Cp>\r\n\u003Cimg src=\"/images/Pedestrian-Readout.png\" alt=\"Readout of pedestrian detection\" style=\"max-width:50%; display:block; margin:auto;\" />\r\n\u003C/p>\r\n\r\n\u003Ch4>Offroading\u003C/h4>\r\n\u003Cp>\r\nThe offroad section consists of five simple phases. First, the robot drives forwards at an angle towards the hill for a set period of sim time. It’s not particularly sensitive as to where the robot ends up, so this worked fine. Then the robot would turn until it detected the windows of the car through the blue mask shown in the figure below. These windows are a very particular shade of blue, so it doesn’t get confused by the clue boards. It then PIDs directly toward the centroid of these pixels until they make up a certain percentage of the screen. Finally, it turns right until the pink line is centred in its view, thus catching a view of the clue, and drives towards the tunnel.\r\n\u003C/p>\r\n\u003Cp>\r\n\u003Cimg src=\"/images/Blue_Filter.png\" alt=\"Filtering for car windows\" style=\"max-width:40%; display:block; margin:auto;\" />\r\n\u003C/p>\r\n\r\n\u003Ch4>Emergency Controller\u003C/h4>\r\n\u003Cp>\r\nThe controller used in the competition does the following:\u003Cbr>\r\nDrive for a predetermined time in predetermined steps, wait for the CNN to report a board, and then continue its circuit. This circuit includes teleportation to each staging line. \r\n\u003Ca href=\"https://github.com/AmjadYa/fizzcomp/blob/main/src/controller/src/bismillah_sequence.py\" target=\"_blank\">Here is a link to the python file which contains this sequence.\u003C/a>\r\n\u003C/p>\r\n\r\n\u003Ch3>Clue Plate Recognition Module (CNN)\u003C/h3>\r\n\r\n\u003Ch4>Data Acquisition\u003C/h4>\r\n\u003Cp>\r\nAll data was acquired manually by going into the simulation after changing the clue boards. I would drive around and screenshot them using the GUI. Here’s a demo of what that looked like:\r\n\u003C/p>\r\n\u003Cp>\r\n\u003Cimg src=\"/images/usingui.png\" alt=\"GUI showing homography for screenshots\" style=\"max-width:85%; display:block; margin:auto;\" />\r\n\u003C/p>\r\n\u003Cp>\r\n\u003Ca href=\"https://github.com/AmjadYa/fizzcomp/tree/main/src/controller/saved_images\" target=\"_blank\">This was done tens of times\u003C/a> – and although time consuming – gave us very transferrable (as in: relevant to what we would see on competition day) training data. The GUI has drop-down menus that allow you to control what you want to see in the display labels. This was very useful for testing how much to threshold, erode, dilute, and process images in general.\r\n\u003C/p>\r\n\r\n\u003Ch4>Image Processing\u003C/h4>\r\n\u003Cp>\r\nImages were then meticulously broken down into contours and stretched to get clear pictures of each letter for the CNN to train on / predict. You can read the exact steps \r\n\u003Ca href=\"https://github.com/AmjadYa/fizzcomp/blob/main/src/controller/src/prediction_module.py\" target=\"_blank\">here, in the prediction module of the repository.\u003C/a> In short, images were cut in half and a contour box was forcefully drawn around the individual letters. If there was overlap, contours were limited to a certain size and would forcefully split into the best number of boxes in order to separate the letters. \r\n\u003Ca href=\"https://colab.research.google.com/drive/1P0j5OBePBGmmYEO2-LqW9yZ2Fc2BDcIC?usp=sharing\" target=\"_blank\">There is also a more digestible version on Google Colab\u003C/a> which you can play around with and test for yourself.\r\n\u003C/p>\r\n\r\n\u003Ch4>Model Architecture\u003C/h4>\r\n\u003Cp>\r\nThe architecture we used for training was inspired by previous work we did in our labs in class, \r\n\u003Ca href=\"https://docs.google.com/document/d/1fihwZEmhyZtpW0ugc-sJJCCq7CGPNyqaXhZ294aimtY/edit?tab=t.0\" target=\"_blank\">which you can read more about here\u003C/a>. The following is an image of the model summary.\r\n\u003C/p>\r\n\u003Cp>\r\n\u003Cimg src=\"/images/modelarch.png\" alt=\"Model architecture summary\" style=\"max-width:70%; display:block; margin:auto;\" />\r\n\u003C/p>\r\n\r\n\u003Ch4>Training on Images\u003C/h4>\r\n\u003Cp>\r\nSince we did not do any data augmentation (no artificially generated data based on what we had already), we needed as much of our dataset used for training as possible. As such, the strategy was to monitor training using an 80-20 training-validation split, and once it was satisfactory, to remove the validation set entirely and train on the complete set of images.\r\n\u003C/p>\r\n\u003Cp>\r\n\u003Cimg src=\"/images/training.png\" alt=\"Fully using our dataset\" style=\"max-width:70%; display:block; margin:auto;\" />\r\n\u003C/p>\r\n\u003Cp>\r\nNotice in the figure above, \u003Ccode>x_train\u003C/code> and \u003Ccode>y_train\u003C/code> are commented out, this is because we were using all letters to train our final competition model. Below is a sample of what our accuracy, loss and confusion matrix looked like when we actually had a split:\r\n\u003C/p>\r\n\u003Cp>\r\n\u003Cimg src=\"/images/trainval.png\" alt=\"Training and validation graphs\" style=\"max-width:100%; display:block; margin:auto;\" />\r\n\u003C/p>\r\n\u003Cp>\r\n\u003Cimg src=\"/images/confusion.png\" alt=\"Beautiful confusion matrix\" style=\"max-width:100%; display:block; margin:auto;\" />\r\n\u003C/p>\r\n\r\n\u003Ch4>Failure Cases\u003C/h4>\r\n\u003Cp>\r\nOccasionally, when segmenting the letters, there is enough overlap to cause a failed prediction. There was an attempt to solve this by intentionally training on overlapped images, however not enough was manually captured. Above, we discussed our performance during competition. Unluckily, one of our predictions failed as a result of this overlap; however, this is a rare event. \r\n\u003Ca href=\"https://drive.google.com/drive/folders/1jROyw9Q_FamL0aAo_l1EG_6aiC61KTwb?usp=sharing\" target=\"_blank\">You can view the individual letters that were used to train the model here\u003C/a>. You may notice some of them have a lot of overlap (by design).\r\n\u003C/p>\r\n\r\n\u003Chr />\r\n\r\n\u003Ch2>Conclusion\u003C/h2>\r\n\r\n\u003Ch3>Performance During Competition\u003C/h3>\r\n\u003Cp>\r\nThe robot performed almost as well as designed during competition. It correctly reported four different clues, teleported three times, and crossed the line once to a total of 18 points. Due to one misspelled clue, we were not able to maximize our points, but there was no catastrophic malfunction.\r\n\u003C/p>\r\n\r\n\u003Ch3>Unadopted Other Attempts\u003C/h3>\r\n\u003Cp>\r\nNotice, in the figure which shows the GUI, that there is a button labeled “Record” with a red light next to it. This was because due to conflicts integrating driving and clue detection together, I tried to quickly create an imitation learning model to navigate the robot. The record button would take screenshots every 100ms and create a CSV file which mapped the current linear and angular velocity of the robot to the image. Images were then fed into \r\n\u003Ca href=\"https://colab.research.google.com/drive/1jRNEESqv6ywCmLLXx__tK_0oSqThVjV7?usp=sharing\" target=\"_blank\">another CNN\u003C/a> with very similar architecture to the one used for training on letters. Surprisingly this method had accurately navigated the first segment of the road up to the second clue board. However, it became quickly apparent that we did not have the necessary compute (shoddy laptop :/) in order to use an imitation learning model in time for the competition, thus it was scrapped and the emergency teleportation method was used instead.\r\n\u003C/p>\r\n\r\n\u003Ch3>Improvements for Future\u003C/h3>\r\n\u003Cp>\r\nThe primary area in need of improvement is the integration of clue detection and driving. We would have used a very wide angled camera so that the clues could be seen without stopping, and altered the homography such that it could undo the resulting distortion. This would also include getting the robot to reach the top of the hill and ironing out the CNN overlap issue. Secondly, there is a lot of potential to optimize the usage of nodes to separate actions which needed to occur simultaneously on the robot.\r\n\u003C/p>\r\n\r\n\u003Chr />\r\n\r\n\u003Ch2>Appendices\u003C/h2>\r\n\r\n\u003Ch3>Referenced Material\u003C/h3>\r\n\u003Cp>\r\nAll referenced material can be found on the ENPH 353 website. There, you will find access to eight labs which helped us in various ways for this project.\r\n\u003Cbr>\u003Cbr>\r\n\u003Ca href=\"https://projectlab.engphys.ubc.ca/enph-353/\" target=\"_blank\">Here’s a link. It will likely update as months pass.\u003C/a>\r\n\u003C/p>\r\n\r\n\u003Ch3>Notable People\u003C/h3>\r\n\u003Cp>\r\nWe would be remiss not to mention Daniel Song, Michael Khoo and Ebrahim Hussain who gave input regarding their previous experience in this course.\u003Cbr>\u003Cbr>\r\nLastly, many discussions were had with Ella Yan (a colleague taking the course) particularly regarding image processing and how to implement an imitation learning model. We would like to give our flowers to her and her teammate Nora Shao for managing to implement their imitation learning model with severe time constraints.\r\n\u003C/p>\r\n\r\n\u003Ch3>ChatGPT Usage\u003C/h3>\r\n\u003Cp>\r\nChatGPT was used to help implement ideas that we already had, and for debugging. Here is an example of how ChatGPT was used to help us troubleshoot making predictions.\r\n\u003C/p>\r\n\u003Cp>\r\n\u003Cimg src=\"/images/tensor.png\" alt=\"Troubleshooting with ChatGPT\" style=\"max-width:70%; display:block; margin:auto;\" />\r\n\u003C/p>\r\n\u003Cp>\r\nIn fact, this was a notable conversation with ChatGPT, as it helped to reveal that there was a mismatch between the version of the model in Google Colab and the one used locally! (Took a while to figure out what was wrong.)\r\n\u003C/p>","src/data/project/cnn-letter-detecting-ros-sim.md","6a442714321eb778",{"html":100,"metadata":101},"\u003Cdiv class=\"flex gap-2\" style=\"justify-content: center ; align-items: center\">\n    \u003Cvideo src=\"/videos/CNN clue detecting robot.mp4\" place-content-center=\"\" muted style=\"max-height:400px ; object-fit:cover\" controls>\u003C/video>\n\u003C/div>\n\u003Ch2>Introduction\u003C/h2>\n\u003Ch3>Background of the Report\u003C/h3>\n\u003Cp>\nThis project was the culmination of \u003Cstrong>eight\u003C/strong> labs that provided the essential background to develop a fully autonomous, line-following robot for the semester's end competition. This report details the development and implementation of a control system and Convolutional Neural Network (CNN) enabling the robot to detect and read clue plates, navigate a predefined track and avoid obstacles. The labs were very useful! For a personal logbook of all labs and related thoughts, you can click \u003Ca href=\"/pdfs/Amjad%20Yaghi%20individual%20logbook.pdf\" target=\"_blank\">here\u003C/a>.\n\u003C/p>\n\u003Ch3>Contribution Split\u003C/h3>\n\u003Cp>\nRichard Helper: PID-based line following and obstacle avoidance.\u003Cbr>\nAmjad Yaghi: CNN-based clue detection, visual processing of clue boards, UI development, emergency teleportation strategy for competition.\n\u003C/p>\n\u003Ch3>Software Architecture\u003C/h3>\n\u003Cp>\n\u003Cimg src=\"/images/controller.drawio.png\" alt=\"Driving software architecture\" style=\"max-width:80%; display:block; margin:auto;\">\n\u003C/p>\n\u003Cp style=\"text-align:center;\">\n\u003Ca href=\"https://github.com/OdysseusInSpace/ENPH353_LineFollow\" target=\"_blank\">Driving software architecture\u003C/a>\n\u003C/p>\n\u003Cp>\n\u003Cimg src=\"/images/gui3.drawio.png\" alt=\"Software architecture used on competition day\" style=\"max-width:80%; display:block; margin:auto;\">\n\u003C/p>\n\u003Cp style=\"text-align:center;\">\n\u003Ca href=\"https://github.com/AmjadYa/fizzcomp/tree/main\" target=\"_blank\">Software architecture used on competition day\u003C/a>\n\u003C/p>\n\u003Chr>\n\u003Ch2>Discussion\u003C/h2>\n\u003Ch3>Robot Driving Method\u003C/h3>\n\u003Cp>\nThe planned driving module used PID, a state machine, and several specific behaviours to make its way through the obstacle course. Unfortunately, we were not able to integrate it with the clue detection in time, and so were forced to go through with our emergency controller detailed at the end of the section.\n\u003C/p>\n\u003Ch4>Contour Detection and Basic PID\u003C/h4>\n\u003Cp>\nDue to the roundabout in the middle of the on-road section, the robot needed to be biased toward turning left so as not to collide head on with the truck. There was also the issue of noise in the middle of the road during the dirt phase. To counteract these issues, the PID was based off the centre of the leftmost contour above a certain perimeter threshold, using an adaptive offset towards the opposite end of the screen that grew smaller as the marker was placed closer to the horizon. See the figure below for an example.\n\u003C/p>\n\u003Cp>\n\u003Cimg src=\"/images/Dirt_Contour.png\" alt=\"Contour detection and PID marker\" style=\"max-width:50%; display:block; margin:auto;\">\n\u003C/p>\n\u003Ch4>Staging\u003C/h4>\n\u003Cp>\nAs the robot drives, it encounters different scenarios marked by lines across the road. There is a red bar that marks the beginning of the pedestrian section and pink ones that mark the transition between the paved road, dirt road, offroad, and hill sections. The drive module uses a state machine to account for this. Whenever one of these lines is detected – that is, the bottom 10% of the screen has a certain number of pixels that satisfy the red-pink masking – and then disappears, the next state is initialized. This usually means swapping image processing and control algorithms, as is detailed later in this section.\n\u003C/p>\n\u003Ch4>NPC Avoidance\u003C/h4>\n\u003Cp>\nOf the three NPCs, the pedestrian was the only one that the robot would ever hit. The truck acted as a contour in such a way that the PID automatically avoided it, and Baby Yoda was navigated around entirely.\u003Cbr>\nTo dodge the pedestrian, the staging system was put to simple use. Upon crossing the red line, the robot swaps to phase 2, and waits for the marker to shift dramatically. As the pedestrian crosses the left side of the road, it would disrupt the contour and cause this shift. The robot would then resume its normal behaviour.\n\u003C/p>\n\u003Cp>\n\u003Cimg src=\"/images/Pedestrian-Readout.png\" alt=\"Readout of pedestrian detection\" style=\"max-width:50%; display:block; margin:auto;\">\n\u003C/p>\n\u003Ch4>Offroading\u003C/h4>\n\u003Cp>\nThe offroad section consists of five simple phases. First, the robot drives forwards at an angle towards the hill for a set period of sim time. It’s not particularly sensitive as to where the robot ends up, so this worked fine. Then the robot would turn until it detected the windows of the car through the blue mask shown in the figure below. These windows are a very particular shade of blue, so it doesn’t get confused by the clue boards. It then PIDs directly toward the centroid of these pixels until they make up a certain percentage of the screen. Finally, it turns right until the pink line is centred in its view, thus catching a view of the clue, and drives towards the tunnel.\n\u003C/p>\n\u003Cp>\n\u003Cimg src=\"/images/Blue_Filter.png\" alt=\"Filtering for car windows\" style=\"max-width:40%; display:block; margin:auto;\">\n\u003C/p>\n\u003Ch4>Emergency Controller\u003C/h4>\n\u003Cp>\nThe controller used in the competition does the following:\u003Cbr>\nDrive for a predetermined time in predetermined steps, wait for the CNN to report a board, and then continue its circuit. This circuit includes teleportation to each staging line. \n\u003Ca href=\"https://github.com/AmjadYa/fizzcomp/blob/main/src/controller/src/bismillah_sequence.py\" target=\"_blank\">Here is a link to the python file which contains this sequence.\u003C/a>\n\u003C/p>\n\u003Ch3>Clue Plate Recognition Module (CNN)\u003C/h3>\n\u003Ch4>Data Acquisition\u003C/h4>\n\u003Cp>\nAll data was acquired manually by going into the simulation after changing the clue boards. I would drive around and screenshot them using the GUI. Here’s a demo of what that looked like:\n\u003C/p>\n\u003Cp>\n\u003Cimg src=\"/images/usingui.png\" alt=\"GUI showing homography for screenshots\" style=\"max-width:85%; display:block; margin:auto;\">\n\u003C/p>\n\u003Cp>\n\u003Ca href=\"https://github.com/AmjadYa/fizzcomp/tree/main/src/controller/saved_images\" target=\"_blank\">This was done tens of times\u003C/a> – and although time consuming – gave us very transferrable (as in: relevant to what we would see on competition day) training data. The GUI has drop-down menus that allow you to control what you want to see in the display labels. This was very useful for testing how much to threshold, erode, dilute, and process images in general.\n\u003C/p>\n\u003Ch4>Image Processing\u003C/h4>\n\u003Cp>\nImages were then meticulously broken down into contours and stretched to get clear pictures of each letter for the CNN to train on / predict. You can read the exact steps \n\u003Ca href=\"https://github.com/AmjadYa/fizzcomp/blob/main/src/controller/src/prediction_module.py\" target=\"_blank\">here, in the prediction module of the repository.\u003C/a> In short, images were cut in half and a contour box was forcefully drawn around the individual letters. If there was overlap, contours were limited to a certain size and would forcefully split into the best number of boxes in order to separate the letters. \n\u003Ca href=\"https://colab.research.google.com/drive/1P0j5OBePBGmmYEO2-LqW9yZ2Fc2BDcIC?usp=sharing\" target=\"_blank\">There is also a more digestible version on Google Colab\u003C/a> which you can play around with and test for yourself.\n\u003C/p>\n\u003Ch4>Model Architecture\u003C/h4>\n\u003Cp>\nThe architecture we used for training was inspired by previous work we did in our labs in class, \n\u003Ca href=\"https://docs.google.com/document/d/1fihwZEmhyZtpW0ugc-sJJCCq7CGPNyqaXhZ294aimtY/edit?tab=t.0\" target=\"_blank\">which you can read more about here\u003C/a>. The following is an image of the model summary.\n\u003C/p>\n\u003Cp>\n\u003Cimg src=\"/images/modelarch.png\" alt=\"Model architecture summary\" style=\"max-width:70%; display:block; margin:auto;\">\n\u003C/p>\n\u003Ch4>Training on Images\u003C/h4>\n\u003Cp>\nSince we did not do any data augmentation (no artificially generated data based on what we had already), we needed as much of our dataset used for training as possible. As such, the strategy was to monitor training using an 80-20 training-validation split, and once it was satisfactory, to remove the validation set entirely and train on the complete set of images.\n\u003C/p>\n\u003Cp>\n\u003Cimg src=\"/images/training.png\" alt=\"Fully using our dataset\" style=\"max-width:70%; display:block; margin:auto;\">\n\u003C/p>\n\u003Cp>\nNotice in the figure above, \u003Ccode>x_train\u003C/code> and \u003Ccode>y_train\u003C/code> are commented out, this is because we were using all letters to train our final competition model. Below is a sample of what our accuracy, loss and confusion matrix looked like when we actually had a split:\n\u003C/p>\n\u003Cp>\n\u003Cimg src=\"/images/trainval.png\" alt=\"Training and validation graphs\" style=\"max-width:100%; display:block; margin:auto;\">\n\u003C/p>\n\u003Cp>\n\u003Cimg src=\"/images/confusion.png\" alt=\"Beautiful confusion matrix\" style=\"max-width:100%; display:block; margin:auto;\">\n\u003C/p>\n\u003Ch4>Failure Cases\u003C/h4>\n\u003Cp>\nOccasionally, when segmenting the letters, there is enough overlap to cause a failed prediction. There was an attempt to solve this by intentionally training on overlapped images, however not enough was manually captured. Above, we discussed our performance during competition. Unluckily, one of our predictions failed as a result of this overlap; however, this is a rare event. \n\u003Ca href=\"https://drive.google.com/drive/folders/1jROyw9Q_FamL0aAo_l1EG_6aiC61KTwb?usp=sharing\" target=\"_blank\">You can view the individual letters that were used to train the model here\u003C/a>. You may notice some of them have a lot of overlap (by design).\n\u003C/p>\n\u003Chr>\n\u003Ch2>Conclusion\u003C/h2>\n\u003Ch3>Performance During Competition\u003C/h3>\n\u003Cp>\nThe robot performed almost as well as designed during competition. It correctly reported four different clues, teleported three times, and crossed the line once to a total of 18 points. Due to one misspelled clue, we were not able to maximize our points, but there was no catastrophic malfunction.\n\u003C/p>\n\u003Ch3>Unadopted Other Attempts\u003C/h3>\n\u003Cp>\nNotice, in the figure which shows the GUI, that there is a button labeled “Record” with a red light next to it. This was because due to conflicts integrating driving and clue detection together, I tried to quickly create an imitation learning model to navigate the robot. The record button would take screenshots every 100ms and create a CSV file which mapped the current linear and angular velocity of the robot to the image. Images were then fed into \n\u003Ca href=\"https://colab.research.google.com/drive/1jRNEESqv6ywCmLLXx__tK_0oSqThVjV7?usp=sharing\" target=\"_blank\">another CNN\u003C/a> with very similar architecture to the one used for training on letters. Surprisingly this method had accurately navigated the first segment of the road up to the second clue board. However, it became quickly apparent that we did not have the necessary compute (shoddy laptop :/) in order to use an imitation learning model in time for the competition, thus it was scrapped and the emergency teleportation method was used instead.\n\u003C/p>\n\u003Ch3>Improvements for Future\u003C/h3>\n\u003Cp>\nThe primary area in need of improvement is the integration of clue detection and driving. We would have used a very wide angled camera so that the clues could be seen without stopping, and altered the homography such that it could undo the resulting distortion. This would also include getting the robot to reach the top of the hill and ironing out the CNN overlap issue. Secondly, there is a lot of potential to optimize the usage of nodes to separate actions which needed to occur simultaneously on the robot.\n\u003C/p>\n\u003Chr>\n\u003Ch2>Appendices\u003C/h2>\n\u003Ch3>Referenced Material\u003C/h3>\n\u003Cp>\nAll referenced material can be found on the ENPH 353 website. There, you will find access to eight labs which helped us in various ways for this project.\n\u003Cbr>\u003Cbr>\n\u003Ca href=\"https://projectlab.engphys.ubc.ca/enph-353/\" target=\"_blank\">Here’s a link. It will likely update as months pass.\u003C/a>\n\u003C/p>\n\u003Ch3>Notable People\u003C/h3>\n\u003Cp>\nWe would be remiss not to mention Daniel Song, Michael Khoo and Ebrahim Hussain who gave input regarding their previous experience in this course.\u003Cbr>\u003Cbr>\nLastly, many discussions were had with Ella Yan (a colleague taking the course) particularly regarding image processing and how to implement an imitation learning model. We would like to give our flowers to her and her teammate Nora Shao for managing to implement their imitation learning model with severe time constraints.\n\u003C/p>\n\u003Ch3>ChatGPT Usage\u003C/h3>\n\u003Cp>\nChatGPT was used to help implement ideas that we already had, and for debugging. Here is an example of how ChatGPT was used to help us troubleshoot making predictions.\n\u003C/p>\n\u003Cp>\n\u003Cimg src=\"/images/tensor.png\" alt=\"Troubleshooting with ChatGPT\" style=\"max-width:70%; display:block; margin:auto;\">\n\u003C/p>\n\u003Cp>\nIn fact, this was a notable conversation with ChatGPT, as it helped to reveal that there was a mismatch between the version of the model in Google Colab and the one used locally! (Took a while to figure out what was wrong.)\n\u003C/p>",{"headings":102,"imagePaths":103,"frontmatter":92},[],[]]